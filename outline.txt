# Voice Data AI/ML Project - Complete Implementation Outline

## 📁 Project Structure
```
voice_ml_project/
├── data/
│   ├── raw/                    # Raw audio files
│   ├── processed/              # Preprocessed audio
│   └── features/               # Extracted features
├── notebooks/
│   ├── 01_data_exploration.ipynb
│   ├── 02_preprocessing.ipynb
│   ├── 03_feature_extraction.ipynb
│   ├── 04_model_training.ipynb
│   └── 05_evaluation.ipynb
├── src/
│   ├── __init__.py
│   ├── data_loader.py          # Dataset loading utilities
│   ├── preprocessing.py        # Audio preprocessing functions
│   ├── feature_extraction.py   # Feature extraction methods
│   ├── models.py              # ML/DL model definitions
│   ├── utils.py               # Utility functions
│   └── api.py                 # API endpoints
├── models/
│   ├── emotion_classifier.pkl  # Trained emotion model
│   ├── speaker_identifier.pkl  # Trained speaker model
│   └── scaler.pkl             # Feature scaler
├── app/
│   ├── streamlit_app.py       # Streamlit application
│   ├── requirements.txt       # Dependencies
│   └── config.py              # Configuration settings
├── tests/
│   └── test_models.py
├── docs/
│   └── report.md              # Project report
└── README.md
```

## 🎯 Part 1: Dataset Implementation

### Recommended Datasets
1. **RAVDESS** (Emotion Classification)
   - 24 actors, 8 emotions
   - Download: `https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio`

2. **Mozilla Common Voice** (Speaker Identification)
   - Multi-language, diverse speakers
   - Download: `https://commonvoice.mozilla.org/`

### Dataset Loading Code Structure
```python
class VoiceDataset:
    def __init__(self, data_path, dataset_type="ravdess"):
        self.data_path = data_path
        self.dataset_type = dataset_type
        
    def load_ravdess(self):
        # Load RAVDESS with emotion labels
        pass
        
    def load_common_voice(self):
        # Load Common Voice with speaker labels
        pass
```

## 🔧 Part 2: Preprocessing Pipeline

### Audio Preprocessing Functions
```python
def preprocess_audio(audio_path, target_sr=16000):
    """
    - Load audio file
    - Convert to mono
    - Resample to target_sr
    - Trim silences
    - Normalize amplitude
    """
    pass

def batch_preprocess(input_dir, output_dir):
    """Batch process all audio files"""
    pass
```

### Key Preprocessing Steps
- **Loading**: Use `librosa.load()` or `torchaudio.load()`
- **Mono Conversion**: `librosa.to_mono()`
- **Resampling**: `librosa.resample()`
- **Silence Trimming**: `librosa.effects.trim()`
- **Normalization**: Min-max or Z-score normalization

## 🎵 Part 3: Feature Extraction

### Feature Extraction Methods
```python
class FeatureExtractor:
    def extract_mfcc(self, audio, sr, n_mfcc=13):
        """Extract MFCC features"""
        return librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)
    
    def extract_spectrogram(self, audio, sr):
        """Extract mel-spectrogram"""
        return librosa.feature.melspectrogram(y=audio, sr=sr)
    
    def extract_chroma(self, audio, sr):
        """Extract chroma features"""
        return librosa.feature.chroma_stft(y=audio, sr=sr)
    
    def extract_all_features(self, audio, sr):
        """Extract combined feature set"""
        pass
```

### Feature Types to Extract
- **MFCC**: 13 coefficients + deltas
- **Mel-Spectrogram**: 128 mel bands
- **Chroma**: 12 pitch classes
- **Spectral Features**: Centroid, rolloff, zero-crossing rate
- **Statistical Features**: Mean, std, skewness, kurtosis

## 🤖 Part 4: Model Building

### Classical ML Models
```python
class ClassicalModels:
    def __init__(self):
        self.models = {
            'logistic_regression': LogisticRegression(),
            'random_forest': RandomForestClassifier(),
            'svm': SVC(),
            'gradient_boosting': GradientBoostingClassifier()
        }
    
    def train_emotion_classifier(self, X_train, y_train):
        """Train emotion classification models"""
        pass
    
    def train_speaker_identifier(self, X_train, y_train):
        """Train speaker identification models"""
        pass
```

### Deep Learning Models
```python
class DeepLearningModels:
    def build_cnn_model(self, input_shape, num_classes):
        """Build CNN for spectrogram classification"""
        model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
            MaxPooling2D(2, 2),
            Conv2D(64, (3, 3), activation='relu'),
            MaxPooling2D(2, 2),
            Flatten(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(num_classes, activation='softmax')
        ])
        return model
    
    def build_rnn_model(self, input_shape, num_classes):
        """Build RNN for sequence classification"""
        pass
    
    def build_transformer_model(self, input_shape, num_classes):
        """Build Transformer for audio classification"""
        pass
```

## 📊 Part 5: Evaluation Framework

### Evaluation Metrics
```python
class ModelEvaluator:
    def evaluate_model(self, model, X_test, y_test):
        """
        - Accuracy
        - Precision, Recall, F1-score
        - Confusion Matrix
        - Classification Report
        """
        predictions = model.predict(X_test)
        
        metrics = {
            'accuracy': accuracy_score(y_test, predictions),
            'classification_report': classification_report(y_test, predictions),
            'confusion_matrix': confusion_matrix(y_test, predictions)
        }
        return metrics
    
    def compare_models(self, models, X_test, y_test):
        """Compare multiple models performance"""
        pass
```

### Visualization Functions
```python
def plot_confusion_matrix(cm, class_names):
    """Plot confusion matrix heatmap"""
    pass

def plot_feature_importance(model, feature_names):
    """Plot feature importance for tree-based models"""
    pass

def plot_spectrogram(audio, sr):
    """Visualize audio spectrogram"""
    pass
```

## 🚀 Part 6: API Endpoints

### FastAPI Backend
```python
from fastapi import FastAPI, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
import joblib
import librosa
import numpy as np

app = FastAPI(title="Voice Analysis API")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load trained models
emotion_model = joblib.load('models/emotion_classifier.pkl')
speaker_model = joblib.load('models/speaker_identifier.pkl')
scaler = joblib.load('models/scaler.pkl')

@app.post("/predict/emotion")
async def predict_emotion(file: UploadFile = File(...)):
    """
    Predict emotion from uploaded audio file
    Returns: {"emotion": "happy", "confidence": 0.85}
    """
    try:
        # Load audio file
        audio_bytes = await file.read()
        audio, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)
        
        # Extract features
        features = extract_features(audio, sr)
        features_scaled = scaler.transform([features])
        
        # Make prediction
        prediction = emotion_model.predict(features_scaled)[0]
        confidence = emotion_model.predict_proba(features_scaled).max()
        
        return {
            "emotion": prediction,
            "confidence": float(confidence),
            "status": "success"
        }
    except Exception as e:
        return {"error": str(e), "status": "error"}

@app.post("/predict/speaker")
async def predict_speaker(file: UploadFile = File(...)):
    """
    Predict speaker from uploaded audio file
    Returns: {"speaker_id": "speaker_001", "confidence": 0.92}
    """
    try:
        # Similar implementation as emotion prediction
        audio_bytes = await file.read()
        audio, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)
        
        features = extract_features(audio, sr)
        features_scaled = scaler.transform([features])
        
        prediction = speaker_model.predict(features_scaled)[0]
        confidence = speaker_model.predict_proba(features_scaled).max()
        
        return {
            "speaker_id": prediction,
            "confidence": float(confidence),
            "status": "success"
        }
    except Exception as e:
        return {"error": str(e), "status": "error"}

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy"}

@app.get("/models/info")
async def get_model_info():
    """Get information about loaded models"""
    return {
        "emotion_classes": ["happy", "sad", "angry", "neutral", "fearful", "disgusted", "surprised"],
        "speaker_count": 24,
        "feature_count": 40
    }

def extract_features(audio, sr):
    """Extract features for model prediction"""
    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13).mean(axis=1)
    chroma = librosa.feature.chroma_stft(y=audio, sr=sr).mean(axis=1)
    mel = librosa.feature.melspectrogram(y=audio, sr=sr).mean(axis=1)
    
    features = np.concatenate([mfcc, chroma, mel])
    return features

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## 🎨 Streamlit Application

### Main Streamlit App
```python
import streamlit as st
import requests
import librosa
import matplotlib.pyplot as plt
import numpy as np
import io
import soundfile as sf
from audio_recorder_streamlit import audio_recorder

# Configure page
st.set_page_config(
    page_title="Voice Analysis App",
    page_icon="🎤",
    layout="wide"
)

# API Configuration
API_BASE_URL = "http://localhost:8000"

def main():
    st.title("🎤 Voice Analysis Application")
    st.markdown("Upload or record audio to analyze emotions and identify speakers")
    
    # Sidebar for model selection
    st.sidebar.header("Analysis Options")
    analysis_type = st.sidebar.selectbox(
        "Select Analysis Type",
        ["Emotion Detection", "Speaker Identification", "Both"]
    )
    
    # Main content area
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("Audio Input")
        
        # Audio input options
        input_method = st.radio(
            "Choose input method:",
            ["Upload File", "Record Audio"]
        )
        
        audio_data = None
        
        if input_method == "Upload File":
            uploaded_file = st.file_uploader(
                "Choose an audio file",
                type=['wav', 'mp3', 'flac', 'm4a']
            )
            if uploaded_file is not None:
                audio_data = uploaded_file
                
        elif input_method == "Record Audio":
            st.write("Click to start/stop recording:")
            audio_bytes = audio_recorder(
                text="Click to record",
                recording_color="#e8b62c",
                neutral_color="#6aa36f",
                icon_name="microphone",
                icon_size="2x"
            )
            
            if audio_bytes is not None:
                audio_data = io.BytesIO(audio_bytes)
        
        # Process audio if available
        if audio_data is not None:
            process_audio(audio_data, analysis_type)
    
    with col2:
        st.header("Model Information")
        display_model_info()

def process_audio(audio_data, analysis_type):
    """Process uploaded/recorded audio"""
    
    st.header("Audio Analysis Results")
    
    # Display audio player
    if hasattr(audio_data, 'read'):
        audio_bytes = audio_data.read()
        audio_data.seek(0)  # Reset pointer
    else:
        audio_bytes = audio_data
    
    st.audio(audio_bytes, format='audio/wav')
    
    # Visualize waveform and spectrogram
    display_audio_visualizations(audio_bytes)
    
    # Make predictions
    with st.spinner("Analyzing audio..."):
        if analysis_type in ["Emotion Detection", "Both"]:
            emotion_result = predict_emotion(audio_data)
            display_emotion_results(emotion_result)
        
        if analysis_type in ["Speaker Identification", "Both"]:
            speaker_result = predict_speaker(audio_data)
            display_speaker_results(speaker_result)

def predict_emotion(audio_data):
    """Call emotion prediction API"""
    try:
        files = {"file": audio_data}
        response = requests.post(f"{API_BASE_URL}/predict/emotion", files=files)
        return response.json()
    except Exception as e:
        st.error(f"Error calling emotion prediction API: {e}")
        return None

def predict_speaker(audio_data):
    """Call speaker identification API"""
    try:
        files = {"file": audio_data}
        response = requests.post(f"{API_BASE_URL}/predict/speaker", files=files)
        return response.json()
    except Exception as e:
        st.error(f"Error calling speaker identification API: {e}")
        return None

def display_emotion_results(result):
    """Display emotion prediction results"""
    if result and result.get("status") == "success":
        st.subheader("🎭 Emotion Detection")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Detected Emotion", result["emotion"])
        with col2:
            st.metric("Confidence", f"{result['confidence']:.2%}")
        
        # Emotion confidence bar chart
        emotions = ["happy", "sad", "angry", "neutral", "fearful", "disgusted", "surprised"]
        # This would need actual probability distribution from API
        st.bar_chart({"Confidence": [result["confidence"] if emotion == result["emotion"] else 0.1 for emotion in emotions]})
    else:
        st.error("Failed to analyze emotion")

def display_speaker_results(result):
    """Display speaker identification results"""
    if result and result.get("status") == "success":
        st.subheader("👤 Speaker Identification")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Speaker ID", result["speaker_id"])
        with col2:
            st.metric("Confidence", f"{result['confidence']:.2%}")
    else:
        st.error("Failed to identify speaker")

def display_audio_visualizations(audio_bytes):
    """Display waveform and spectrogram"""
    try:
        # Load audio using librosa
        audio, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)
        
        # Create visualizations
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))
        
        # Waveform
        time = np.linspace(0, len(audio)/sr, len(audio))
        ax1.plot(time, audio)
        ax1.set_title("Waveform")
        ax1.set_xlabel("Time (s)")
        ax1.set_ylabel("Amplitude")
        
        # Spectrogram
        D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)
        img = librosa.display.specshow(D, y_axis='hz', x_axis='time', ax=ax2)
        ax2.set_title("Spectrogram")
        plt.colorbar(img, ax=ax2)
        
        st.pyplot(fig)
        
    except Exception as e:
        st.error(f"Error creating visualizations: {e}")

def display_model_info():
    """Display model information from API"""
    try:
        response = requests.get(f"{API_BASE_URL}/models/info")
        if response.status_code == 200:
            info = response.json()
            st.write("**Emotion Classes:**")
            for emotion in info["emotion_classes"]:
                st.write(f"- {emotion.title()}")
            
            st.write(f"**Speaker Count:** {info['speaker_count']}")
            st.write(f"**Feature Count:** {info['feature_count']}")
        else:
            st.warning("Could not fetch model information")
    except:
        st.warning("API not available")

# Sidebar with additional options
def sidebar_options():
    st.sidebar.header("Settings")
    
    # Audio preprocessing options
    st.sidebar.subheader("Preprocessing")
    sample_rate = st.sidebar.selectbox("Sample Rate", [16000, 22050, 44100], index=0)
    
    # Model options
    st.sidebar.subheader("Model Selection")
    emotion_model = st.sidebar.selectbox("Emotion Model", ["Random Forest", "CNN", "RNN"])
    speaker_model = st.sidebar.selectbox("Speaker Model", ["SVM", "CNN", "Transformer"])

if __name__ == "__main__":
    main()
```

## 📋 Requirements.txt
```
streamlit>=1.28.0
librosa>=0.10.0
soundfile>=0.12.0
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
matplotlib>=3.7.0
seaborn>=0.12.0
fastapi>=0.104.0
uvicorn>=0.24.0
python-multipart>=0.0.6
audio-recorder-streamlit>=0.0.8
tensorflow>=2.13.0
torch>=2.0.0
torchaudio>=2.0.0
joblib>=1.3.0
requests>=2.31.0
```

## 🚀 Deployment Instructions

### Local Development
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Start FastAPI backend
python src/api.py

# 3. Start Streamlit app (in another terminal)
streamlit run app/streamlit_app.py
```

### Docker Deployment
```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8501 8000

CMD ["streamlit", "run", "app/streamlit_app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

## 📊 Expected Deliverables

### 1. Jupyter Notebooks
- **01_data_exploration.ipynb**: Dataset analysis and visualization
- **02_preprocessing.ipynb**: Audio preprocessing pipeline
- **03_feature_extraction.ipynb**: Feature extraction and analysis
- **04_model_training.ipynb**: Model training and comparison
- **05_evaluation.ipynb**: Model evaluation and results

### 2. Visualizations
- Waveform plots
- Spectrograms and mel-spectrograms
- Feature importance plots
- Confusion matrices
- ROC curves
- Training history plots

### 3. Final Report Structure
```markdown
# Voice Analysis ML Project Report

## Executive Summary
## Dataset Description
## Methodology
  - Preprocessing Pipeline
  - Feature Engineering
  - Model Architecture
## Results
  - Classical ML Performance
  - Deep Learning Performance
  - Model Comparison
## Challenges and Solutions
## Future Improvements
## Conclusion
```

### 4. Interactive Demo
- Streamlit web application
- Real-time audio recording
- File upload capability
- Model predictions with confidence scores
- Audio visualizations

## 🎯 Success Metrics
- **Emotion Classification**: >80% accuracy
- **Speaker Identification**: >85% accuracy
- **API Response Time**: <2 seconds
- **App Loading Time**: <5 seconds

This comprehensive outline provides a complete roadmap for implementing your voice analysis ML project with both classical and deep learning approaches, complete API backend, and interactive Streamlit frontend.